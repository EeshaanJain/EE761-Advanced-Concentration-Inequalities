\chapter{Continuing Martingale Concentrations \& Talagrand's Inequality}

\begin{proof}
We prove for one side of the tail, the other side is corollary. We have 
\begin{align*}
\mu - m = \EE[Y] - m &= \EE[Y-m] \leq \EE[\max\{Y_m, 0\} \\
&= \sum_t \ov{F}_{Y-m}(t) \quad \text{(can also be $\textstyle\int$)} \\
&\leq \int_0^\infty ae^{-t^2/b}dt = \frac{\sqrt{\pi}}{2} a\sqrt{b}
\end{align*}
For the other side, we can take $Z = -Y$, and repeat the same process.
\end{proof}

\section{Extension: BDC outside a bad set}
\begin{theorem}
Let $f$ be a bounded function of $(X_1, \dots, X_n)$ with $m \leq f(X_1, \dots, X_n) \leq M$. Let $\Bcal$ be a bad event and $Z = f(X_1, \dots, X_n)$. Let 
\[
|\EE[Z|\ov{X}_{i-1}, X_i=a, \Bcal^c] - \EE[Z|\ov{X}_{i-1}, X_i=a', \Bcal^c]| \leq c_i \; \forall i
\]
Define, $c^2 = \sum_{i=1}^n c_i^2$. Then,
\begin{equation}
    \begin{split}
        &\PP(Z > \EE[Z] + t + (M-m)\PP(\Bcal)) \\
        &\PP(Z < \EE[Z] - t - (M-m)\PP(\Bcal))
    \end{split}
    \leq \exp\left(-\frac{2t^2}{c}\right) + \PP(\Bcal)
\end{equation}
\end{theorem}
\begin{proof}
We have $\PP(Z > \EE[Z] + t) = \PP(Z > \EE[Z] + t | \Bcal^c)\PP(\Bcal^c) + \PP(Z > \EE[Z] + t | \Bcal)\PP(\Bcal)$. Note that $\PP(Z > \EE[Z] + t | \Bcal) \leq 1$, and $\PP(\Bcal^c) \leq 1$. We can show that the Azuma-Hoeffding holds in the conditioned universe (due to the average Lipschitz condition being held). Thus,
\[
\PP(Z  > \EE[Z|\Bcal^c] + t | \Bcal^c) \leq \exp\left(-\frac{2t^2}{\sum_i c_i^2}\right) = \exp\left(-\frac{2t^2}{c}\right)
\]
Now, we need to bound $\|\EE[Z] - \EE[Z|\Bcal^c]\|$. To do so,
\begin{align*}
    \EE[Z] &= \EE[Z|\Bcal^c] \PP(\Bcal^c) + \EE[Z | \Bcal] \PP(\Bcal) \\
\implies \EE[Z] - \EE[Z|\Bcal^c] &= (\EE[Z|\Bcal^c] - \EE[Z|\Bcal])\PP(\Bcal)
\end{align*}
Since $f(\cdot)$ is bounded, we can write
\[
\EE[Z] - \EE[Z|\Bcal^c] \leq (M-m)\PP(\Bcal)
\]
We can substitute it back into the bound and get the required proof.
\end{proof}
\begin{theorem}
In the same setting as above, the following holds:
\begin{equation}
    \begin{split}
        &\PP(Z > \EE[Z] + t) \\
        &\PP(Z < \EE[Z] - t)
    \end{split}
    \leq \exp\left(-\frac{2t^2}{c}\right) + \PP(\Bcal)
\end{equation}
\end{theorem}
\begin{proof}
Proof omitted.
\end{proof}
\section{Variance Bound for Martingales}
\begin{theorem}
Let $\{Z_0, \dots, Z_n\}$ be a martingale w.r.t $\{X_0, \dots, X_n\}$ with
\begin{enumerate}
    \item $|Z_i - Z_{i-1}| \leq c_i$
    \item $v_i = \sup_{\ov{X}_{i-1}} \text{Var}(Z_i - Z_{i-1} | \ov{X}_{i-1})$ for $i=1,\dots,n$
\end{enumerate}
Let $v = \sum_{i} v_i$. Then,
\begin{equation}
\begin{split}
    &\PP(Zn > Z_0 + t) \\ 
    &\PP(Z_n < Z_0 - t)
\end{split}\;\leq \exp\left(-\frac{t^2}{4v}\right)
\end{equation}
for $0 \leq t \leq 2v/\max_{1\leq i \leq n} c_i$.
\end{theorem}
\begin{proof}
Let $Z_0 = 0$ (WLOG), and $D_i = Z_i - Z_{i-1}$ for $i = 1, \dots, n$. We have $\EE[D_i] = \EE[\EE[Z_i - Z_{i-1} | \ov{X}_{i-1}]] = \EE[Z_{i-1} - Z_{i-1}] = 0$. 
\begin{align*}
\PP(Z_n > t) &\leq \EE\left[e^{\theta Z_n}\right]e^{-\theta t} \\
\EE[e^{\theta Z_n}] &= \EE\left[e^{\theta(D_n + Z_{n-1})}\right] \\
&= \EE\left[\EE\left[e^{\theta (D_n + Z_{n-1})} \big| \ov{X}_{n-1}\right]\right] \\
&= \EE\left[e^{\theta Z_{n-1}} {\EE\left[e^{\theta D_n} \big| \ov{X}_{n-1}\right]}\right]
\end{align*}
We would like to bound the inner expectation. Let $\theta = t/2v$, where $t \leq 2v/\max_i c_i$. Recall that $|D_i| \leq c_i$. We claim: $|\theta D_i| \leq 1$. This is easy to see as $\theta \leq \frac{1}{\max_i c_i}$. Recall that $e^x \leq 1 + x + x^2$ for $|x| \leq 1$. Hence,
\[
\EE\left[e^{\theta D_i} | \ov{X}_{i-1}\right] \leq 1 + \theta \underbrace{\EE[D_i | \ov{X}_{i-1}]}_{=0} + \theta^2 \underbrace{\EE[D_i | \ov{X}_{i-1}]^2}_{\leq v_i}
\]
Thus, $\EE[e^{\theta D_i}|\ov{X}_{i-1}] \leq 1 + \theta^2 v_i \leq e^{\theta^2 v_i}$. This gives us,
\[
\EE\left[e^{\theta Z_n}\right] \leq e^{\theta^2 v_n}\EE\left[e^{\theta Z_{n-1}}\right]
\]
We can repeat this process $n$ times to get,
\[
\EE[e^{\theta Z_n}] \leq e^{\theta^2 \sum_{i=1}^n v_i} = e^{\theta^2 v}
\]
This gives us that $\PP(Z_n > t) \leq e^{-\theta t + \theta^2 v}$. Put $\theta = t/2v$ to get the result (note we cannot minimize over $\theta$, as we picked $\theta$ to get this value).
\end{proof}
\section{Talagrand's Inequality}
\begin{note}
    It will remind you of the blowing-up lemma, but it is more powerful. Moreover, all norms in Talagrand's formulation are $L2$.
\end{note}
\begin{definition}[Talagrand's convex distance]
We define the Talagrand's convex distance as 
\begin{equation}
    d_T(\ov x, \ov y) = \sup_{\{\ov\alpha: \|\ov\alpha\| = 1\}} d_{\ov \alpha} (\ov x, \ov y) = \sup_{\{\ov\alpha: \|\ov\alpha\| = 1\} }\sum_{i=1}^n \alpha_i \bm{1}_{\{x_i \neq y_i\}}
\end{equation}
Consider $\frac{1}{\sqrt n} d(\ov x, \ov y)$. Talagrand's convex distance will always be larger than this.
\end{definition}
\begin{theorem}[Talagrand's Inequality]\label{thm:talagrand-inequality}
Let $\ov X = (X_1, \dots, X_n)$ be independent random variables, and $A \subseteq \Omega$. Then, for any $t \geq 0$,
\[
\PP(\ov X \in A) \PP(d_T(\ov X, A) \geq t) \leq \exp\left(-\frac{t^2}{4}\right)
\]
\end{theorem}
\begin{remark}
We are letting the LHS be larger due to the Talagrand distance, but the product still has the same bound.
\end{remark}